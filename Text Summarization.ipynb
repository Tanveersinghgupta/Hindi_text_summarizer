{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81cd5ae",
   "metadata": {},
   "source": [
    "# Extractive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380842bd",
   "metadata": {},
   "source": [
    "Frequency Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853426a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re #regular expressions\n",
    "import nltk #nl toolkit\n",
    "import string\n",
    "nltk.download('punkt') #tokenizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fa6ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=\"\"\"Artificial Intelligence is human like intelligence. \n",
    "It is the study of itelligent artificial agents. Science and engineering to produce intelligent machines.\n",
    "Solve problems and have intelligence. Related to intelligent behavior. Developing of reasoning machines.\n",
    "Learn from mistakes and successes. Artificial intelligence is related to reasoning in everyday situations\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa829534",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=re.sub(r'\\s+',' ',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac20ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa3f962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d29b25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt=re.sub(r'\\s+',' ',txt)\n",
    "    formatted_text=txt.lower()\n",
    "    tokens=[]\n",
    "    for token in nltk.word_tokenize(formatted_text):\n",
    "        tokens.append(token)\n",
    "    tokens=[word for word in tokens if word not in stopwords and word not in string.punctuation]\n",
    "    formatted_text=' '.join(element for element in tokens)\n",
    "    return (formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898ffdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artificial intelligence human like intelligence study itelligent artificial agents science engineering produce intelligent machines solve problems intelligence related intelligent behavior developing reasoning machines learn mistakes successes artificial intelligence related reasoning everyday situations'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_text=preprocess(txt);formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940577a4",
   "metadata": {},
   "source": [
    "word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679a49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(formatted_text):\n",
    "    frequency=nltk.FreqDist(nltk.word_tokenize(formatted_text))\n",
    "    highest_frequency=max(frequency.values())\n",
    "    for word in frequency.keys():\n",
    "        frequency[word]=(frequency[word]/highest_frequency)\n",
    "    return(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f907839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency=word_frequency(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc57b8b",
   "metadata": {},
   "source": [
    "sentence tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e20f71ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_txt=\"\"\"Artificial Intelligence is human like intelligence. \n",
    "It is the study of itelligent artificial agents. Science and engineering to produce intelligent machines.\n",
    "Solve problems and have intelligence. Related to intelligent behavior. Developing of reasoning machines.\n",
    "Learn from mistakes and successes. Artificial intelligence is related to reasoning in everyday situations\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ec4b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(og_txt):\n",
    "    score_sentences={}\n",
    "    sentence_list=nltk.sent_tokenize(og_txt)\n",
    "    for sentence in sentence_list:\n",
    "        #print(sentence)\n",
    "        for word in nltk.word_tokenize(sentence.lower()):\n",
    "            #print(word)\n",
    "            if sentence not in score_sentences.keys():\n",
    "                score_sentences[sentence]=word_frequency[word]\n",
    "            else:\n",
    "                score_sentences[sentence]+=word_frequency[word]\n",
    "    return(score_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f98a8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sentences=score_sentence(og_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abcf61e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Artificial Intelligence is human like intelligence.': 3.25,\n",
       " 'It is the study of itelligent artificial agents.': 1.5,\n",
       " 'Science and engineering to produce intelligent machines.': 1.75,\n",
       " 'Solve problems and have intelligence.': 1.5,\n",
       " 'Related to intelligent behavior.': 1.25,\n",
       " 'Developing of reasoning machines.': 1.25,\n",
       " 'Learn from mistakes and successes.': 0.75,\n",
       " 'Artificial intelligence is related to reasoning in everyday situations': 3.25}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a7cc634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "best_sentences=heapq.nlargest(3,score_sentence(og_txt),key=score_sentences.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1c30e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence is human like intelligence. Artificial intelligence is related to reasoning in everyday situations Science and engineering to produce intelligent machines.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sentences\n",
    "summary=' '.join(best_sentences)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f6fdfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Summary</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " <mark>Artificial Intelligence is human like intelligence.</mark> It is the study of itelligent artificial agents. <mark>Science and engineering to produce intelligent machines.</mark> Solve problems and have intelligence. Related to intelligent behavior. Developing of reasoning machines. Learn from mistakes and successes. <mark>Artificial intelligence is related to reasoning in everyday situations</mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "txt=''\n",
    "display(HTML(f'<h2>Summary</h2>' ))\n",
    "sentence_list=nltk.sent_tokenize(og_txt)\n",
    "for sentence in sentence_list:\n",
    "    #print(sentence)\n",
    "    #txt += sentence\n",
    "    if sentence in best_sentences:\n",
    "        txt+=' '+sentence.replace(sentence,f\"<mark>{sentence}</mark>\")\n",
    "    else:\n",
    "        txt+=' '+sentence\n",
    "display(HTML(f\"\"\"{txt}\"\"\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44ff62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directly from internet\n",
    "from goose3 import Goose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3da8b4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Automatic_summarization (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000026D7E560F70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    175\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m             raise NewConnectionError(\n\u001b[0m\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x0000026D7E560F70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    490\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    788\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Automatic_summarization (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000026D7E560F70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12784\\2939619434.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGoose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'https://en.wikipedia.org/wiki/Automatic_summarization'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0marticle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\goose3\\__init__.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(self, url, raw_html)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mcrawl_candidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrawlCandidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_html\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__crawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrawl_candidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshutdown_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\goose3\\__init__.py\u001b[0m in \u001b[0;36m__crawl\u001b[1;34m(self, crawl_candidate)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mparsers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavailable_parsers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcrawler_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrawl_candidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\goose3\\__init__.py\u001b[0m in \u001b[0;36mcrawler_wrapper\u001b[1;34m(parser, parsers, crawl_candidate)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                 \u001b[0mcrawler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetcher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                 \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrawl_candidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Parser %s failed to parse the content\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\goose3\\crawler.py\u001b[0m in \u001b[0;36mcrawl\u001b[1;34m(self, crawl_candidate)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;31m# raw html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mraw_html\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrawl_candidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_candidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraw_html\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\goose3\\crawler.py\u001b[0m in \u001b[0;36mget_html\u001b[1;34m(self, crawl_candidate, parsing_candidate)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;31m# fetch HTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fetching html from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrawl_candidate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsing_candidate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ISO-8859-1\"\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# requests has a good idea; use what it says\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;31m# return response as a unicode string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\goose3\\network.py\u001b[0m in \u001b[0;36mfetch_obj\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         response = self._connection.get(\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhttp_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"allow_redirects\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GET\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    585\u001b[0m         }\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    563\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Automatic_summarization (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000026D7E560F70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "g=Goose()\n",
    "url='https://en.wikipedia.org/wiki/Automatic_summarization'\n",
    "article=g.extract(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ae4d610",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12784\\3709316493.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'article' is not defined"
     ]
    }
   ],
   "source": [
    "article.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31cbbc46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12784\\3155714538.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'article' is not defined"
     ]
    }
   ],
   "source": [
    "article.cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95f3d307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35176"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article.cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fe586b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_article=preprocess(article.cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0f44efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"automatic summarization process shortening set data computationally create subset summary represents important relevant information within original content artificial intelligence algorithms commonly developed employed achieve specialized different types data text summarization usually implemented natural language processing methods designed locate informative sentences given document 1 hand visual content summarized using computer vision algorithms image summarization subject ongoing research existing approaches typically attempt display representative images given image collection generate video includes important content entire collection 2 3 4 video summarization algorithms identify extract original video content important frames key-frames and/or important video segments key-shots normally temporally ordered fashion 5 6 7 8 video summaries simply retain carefully selected subset original video frames therefore identical output video synopsis algorithms new video frames synthesized based original video content two general approaches automatic summarization extraction abstraction content extracted original data extracted content modified way examples extracted content include key-phrases used `` tag '' index text document key sentences including headings collectively comprise abstract representative images video segments stated text extraction analogous process skimming summary available headings subheadings figures first last paragraphs section optionally first last sentences paragraph read one chooses read entire document detail 10 examples extraction include key sequences text terms clinical relevance including patient/problem intervention outcome 11 abstractive summarization methods generate new text exist original text 12 applied mainly text abstractive methods build internal semantic representation original content often called language model use representation create summary closer human might express abstraction may transform extracted content paraphrasing sections source document condense text strongly extraction transformation however computationally much challenging extraction involving natural language processing often deep understanding domain original text cases original document relates special field knowledge `` paraphrasing '' even difficult apply image video summarization systems extractive approaches aimed higher summarization quality rely combined software human effort machine aided human summarization extractive techniques highlight candidate passages inclusion human adds removes text human aided machine summarization human post-processes software output way one edits output automatic translation google translate broadly two types extractive summarization tasks depending summarization program focuses first generic summarization focuses obtaining generic summary abstract collection whether documents sets images videos news stories etc. second query relevant summarization sometimes called query-based summarization summarizes objects specific query summarization systems able create query relevant text summaries generic machine-generated summaries depending user needs example summarization problem document summarization attempts automatically produce abstract given document sometimes one might interested generating summary single source document others use multiple source documents example cluster articles topic problem called multi-document summarization related application summarizing news articles imagine system automatically pulls together news articles given topic web concisely represents latest news summary image collection summarization another application example automatic summarization consists selecting representative set images larger set images 13 summary context useful show representative images results image collection exploration system video summarization related domain system automatically creates trailer long video also applications consumer personal videos one might want skip boring repetitive actions similarly surveillance videos one would want extract important suspicious activity ignoring boring redundant frames captured high level summarization algorithms try find subsets objects like set sentences set images cover information entire set also called core-set algorithms model notions like diversity coverage information representativeness summary query based summarization techniques additionally model relevance summary query techniques algorithms naturally model summarization problems textrank pagerank submodular set function determinantal point process maximal marginal relevance mmr etc task following given piece text journal article must produce list keywords key phrase capture primary topics discussed text 14 case research articles many authors provide manually assigned keywords text lacks pre-existing keyphrases example news articles rarely keyphrases attached would useful able automatically number applications discussed consider example text news article keyphrase extractor might select `` army corps engineers '' `` president bush '' `` new orleans '' `` defective flood-control pumps '' keyphrases pulled directly text contrast abstractive keyphrase system would somehow internalize content generate keyphrases appear text closely resemble human might produce `` political negligence '' `` inadequate protection floods '' abstraction requires deep understanding text makes difficult computer system keyphrases many applications enable document browsing providing short summary improve information retrieval documents keyphrases assigned user could search keyphrase produce reliable hits full-text search employed generating index entries large text corpus depending different literature definition key terms words phrases keyword extraction highly related theme beginning work turney 15 many researchers approached keyphrase extraction supervised machine learning problem given document construct example unigram bigram trigram found text though text units also possible discussed compute various features describing example e.g. phrase begin upper-case letter assume known keyphrases available set training documents using known keyphrases assign positive negative labels examples learn classifier discriminate positive negative examples function features classifiers make binary classification test example others assign probability keyphrase instance text might learn rule says phrases initial capital letters likely keyphrases training learner select keyphrases test documents following manner apply example-generation strategy test documents run example learner determine keyphrases looking binary classification decisions probabilities returned learned model probabilities given threshold used select keyphrases keyphrase extractors generally evaluated using precision recall precision measures many proposed keyphrases actually correct recall measures many true keyphrases system proposed two measures combined f-score harmonic mean two f 2pr/ p r matches proposed keyphrases known keyphrases checked stemming applying text normalization designing supervised keyphrase extraction system involves deciding several choices apply unsupervised first choice exactly generate examples turney others used possible unigrams bigrams trigrams without intervening punctuation removing stopwords hulth showed get improvement selecting examples sequences tokens match certain patterns part-of-speech tags ideally mechanism generating examples produces known labeled keyphrases candidates though often case example use unigrams bigrams trigrams never able extract known keyphrase containing four words thus recall may suffer however generating many examples also lead low precision also need create features describe examples informative enough allow learning algorithm discriminate keyphrases non- keyphrases typically features involve various term frequencies many times phrase appears current text larger corpus length example relative position first occurrence various boolean syntactic features e.g. contains caps etc turney paper used 12 features hulth uses reduced set features found successful kea keyphrase extraction algorithm work derived turney 's seminal paper end system need return list keyphrases test document need way limit number ensemble methods i.e. using votes several classifiers used produce numeric scores thresholded provide user-provided number keyphrases technique used turney c4.5 decision trees hulth used single binary classifier learning algorithm implicitly determines appropriate number examples features created need way learn predict keyphrases virtually supervised learning algorithm could used decision trees naive bayes rule induction case turney 's genex algorithm genetic algorithm used learn parameters domain-specific keyphrase extraction algorithm extractor follows series heuristics identify keyphrases genetic algorithm optimizes parameters heuristics respect performance training documents known key phrases another keyphrase extraction algorithm textrank supervised methods nice properties like able produce interpretable rules features characterize keyphrase also require large amount training data many documents known keyphrases needed furthermore training specific domain tends customize extraction process domain resulting classifier necessarily portable turney 's results demonstrate unsupervised keyphrase extraction removes need training data approaches problem different angle instead trying learn explicit features characterize keyphrases textrank algorithm 16 exploits structure text determine keyphrases appear `` central '' text way pagerank selects important web pages recall based notion `` prestige '' `` recommendation '' social networks way textrank rely previous training data rather run arbitrary piece text produce output simply based text 's intrinsic properties thus algorithm easily portable new domains languages textrank general purpose graph-based ranking algorithm nlp essentially runs pagerank graph specially designed particular nlp task keyphrase extraction builds graph using set text units vertices edges based measure semantic lexical similarity text unit vertices unlike pagerank edges typically undirected weighted reflect degree similarity graph constructed used form stochastic matrix combined damping factor `` random surfer model '' ranking vertices obtained finding eigenvector corresponding eigenvalue 1 i.e. stationary distribution random walk graph vertices correspond want rank potentially could something similar supervised methods create vertex unigram bigram trigram etc however keep graph small authors decide rank individual unigrams first step include second step merges highly ranked adjacent unigrams form multi-word phrases nice side effect allowing us produce keyphrases arbitrary length example rank unigrams find `` advanced '' `` natural '' `` language '' `` processing '' get high ranks would look original text see words appear consecutively create final keyphrase using four together note unigrams placed graph filtered part speech authors found adjectives nouns best include thus linguistic knowledge comes play step edges created based word co-occurrence application textrank two vertices connected edge unigrams appear within window size n original text n typically around 2–10 thus `` natural '' `` language '' might linked text nlp `` natural '' `` processing '' would also linked would appear string n words edges build notion `` text cohesion '' idea words appear near likely related meaningful way `` recommend '' reader since method simply ranks individual vertices need way threshold produce limited number keyphrases technique chosen set count user-specified fraction total number vertices graph top vertices/unigrams selected based stationary probabilities post- processing step applied merge adjacent instances unigrams result potentially less final keyphrases produced number roughly proportional length original text initially clear applying pagerank co-occurrence graph would produce useful keyphrases one way think following word appears multiple times throughout text may many different co-occurring neighbors example text machine learning unigram `` learning '' might co-occur `` machine '' `` supervised '' `` un-supervised '' `` semi-supervised '' four different sentences thus `` learning '' vertex would central `` hub '' connects modifying words running pagerank/textrank graph likely rank `` learning '' highly similarly text contains phrase `` supervised classification '' would edge `` supervised '' `` classification '' `` classification '' appears several places thus many neighbors importance would contribute importance `` supervised '' ends high rank selected one top unigrams along `` learning '' probably `` classification '' final post-processing step would end keyphrases `` supervised learning '' `` supervised classification '' short co-occurrence graph contain densely connected regions terms appear often different contexts random walk graph stationary distribution assigns large probabilities terms centers clusters similar densely connected web pages getting ranked highly pagerank approach also used document summarization considered like keyphrase extraction document summarization aims identify essence text real difference dealing larger text units—whole sentences instead words phrases supervised text summarization much like supervised keyphrase extraction basically collection documents human-generated summaries learn features sentences make good candidates inclusion summary features might include position document i.e. first sentences probably important number words sentence etc main difficulty supervised extractive summarization known summaries must manually created extracting sentences sentences original training document labeled `` summary '' `` summary '' typically people create summaries simply using journal abstracts existing summaries usually sufficient sentences summaries necessarily match sentences original text would difficult assign labels examples training note however natural summaries still used evaluation purposes since rouge-1 evaluation considers unigrams duc 2001 2002 evaluation workshops tno developed sentence extraction system multi-document summarization news domain system based hybrid system using naive bayes classifier statistical language models modeling salience although system exhibited good results researchers wanted explore effectiveness maximum entropy classifier meeting summarization task known robust feature dependencies maximum entropy also applied successfully summarization broadcast news domain promising approach adaptive document/text summarization 17 involves first recognizing text genre applying summarization algorithms optimized genre software created 18 unsupervised approach summarization also quite similar spirit unsupervised keyphrase extraction gets around issue costly training data unsupervised summarization approaches based finding `` centroid '' sentence mean word vector sentences document sentences ranked regard similarity centroid sentence principled way estimate sentence importance using random walks eigenvector centrality lexrank 19 algorithm essentially identical textrank use approach document summarization two methods developed different groups time lexrank simply focused summarization could easily used keyphrase extraction nlp ranking task lexrank textrank graph constructed creating vertex sentence document edges sentences based form semantic similarity content overlap lexrank uses cosine similarity tf-idf vectors textrank uses similar measure based number words two sentences common normalized sentences lengths lexrank paper explored using unweighted edges applying threshold cosine values also experimented using edges weights equal similarity score textrank uses continuous similarity scores weights algorithms sentences ranked applying pagerank resulting graph summary formed combining top ranking sentences using threshold length cutoff limit size summary worth noting textrank applied summarization exactly described lexrank used part larger summarization system mead combines lexrank score stationary probability features like sentence position length using linear combination either user-specified automatically tuned weights case training documents might needed though textrank results show additional features absolutely necessary unlike textrank lexrank applied multi-document summarization multi-document summarization automatic procedure aimed extraction information multiple texts written topic resulting summary report allows individual users professional information consumers quickly familiarize information contained large cluster documents way multi-document summarization systems complementing news aggregators performing next step road coping information overload multi-document summarization may also done response question 20 11 multi-document summarization creates information reports concise comprehensive different opinions put together outlined every topic described multiple perspectives within single document goal brief summary simplify information search cut time pointing relevant source documents comprehensive multi-document summary contain required information hence limiting need accessing original files cases refinement required automatic summaries present information extracted multiple sources algorithmically without editorial touch subjective human intervention thus making completely unbiased dubious – discuss multi-document extractive summarization faces problem redundancy ideally want extract sentences `` central '' i.e. contain main ideas `` diverse '' i.e. differ one another example set news articles event article likely many similar sentences address issue lexrank applies heuristic post-processing step adds sentences rank order discards sentences similar ones already summary method called cross-sentence information subsumption csis methods work based idea sentences `` recommend '' similar sentences reader thus one sentence similar many others likely sentence great importance importance also stems importance sentences `` recommending '' thus get ranked highly placed summary sentence must similar many sentences turn also similar many sentences makes intuitive sense allows algorithms applied arbitrary new text methods domain-independent easily portable one could imagine features indicating important sentences news domain might vary considerably biomedical domain however unsupervised `` recommendation '' -based approach applies domain related method maximal marginal relevance mmr 21 uses general-purpose graph-based ranking algorithm like page/lex/textrank handles `` centrality '' `` diversity '' unified mathematical framework based absorbing markov chain random walks random walk certain states end walk algorithm called grasshopper 22 addition explicitly promoting diversity ranking process grasshopper incorporates prior ranking based sentence position case summarization state art results multi-document summarization obtained using mixtures submodular functions methods achieved state art results document summarization corpora duc 04 07 23 similar results achieved use determinantal point processes special case submodular functions duc-04 24 new method multi-lingual multi-document summarization avoids redundancy generates ideograms represent meaning sentence document evaluates similarity comparing ideogram shape position use word frequency training preprocessing uses two user-supplied parameters equivalence two sentences considered equivalent relevance long desired summary idea submodular set function recently emerged powerful modeling tool various summarization problems submodular functions naturally model notions coverage information representation diversity moreover several important combinatorial optimization problems occur special instances submodular optimization example set cover problem special case submodular optimization since set cover function submodular set cover function attempts find subset objects cover given set concepts example document summarization one would like summary cover important relevant concepts document instance set cover similarly facility location problem special case submodular functions facility location function also naturally models coverage diversity another example submodular optimization problem using determinantal point process model diversity similarly maximum-marginal-relevance procedure also seen instance submodular optimization important models encouraging coverage diversity information submodular moreover submodular functions efficiently combined resulting function still submodular hence one could combine one submodular function models diversity another one models coverage use human supervision learn right model submodular function problem submodular functions fitting problems summarization also admit efficient algorithms optimization example simple greedy algorithm admits constant factor guarantee 25 moreover greedy algorithm extremely simple implement scale large datasets important summarization problems submodular functions achieved state-of-the-art almost summarization problems example work lin bilmes 2012 26 shows submodular functions achieve best results date duc-04 duc-05 duc-06 duc-07 systems document summarization similarly work lin bilmes 2011 27 shows many existing systems automatic summarization instances submodular functions breakthrough result establishing submodular functions right models summarization problems citation needed submodular functions also used summarization tasks tschiatschek et al. 2014 show 28 mixtures submodular functions achieve state-of-the-art results image collection summarization similarly bairi et al. 2015 29 show utility submodular functions summarizing multi-document topic hierarchies submodular functions also successfully used summarizing machine learning datasets 30 • reddit bot `` autotldr '' 31 created 2011 summarizes news articles comment-section reddit posts found useful reddit community upvoted summaries hundreds thousands times 32 name reference tl dr − internet slang `` long n't read '' 33 34 • adversarial stylometry may make use summaries detail lost major summary sufficiently stylistically different input common way evaluate informativeness automatic summaries compare human-made model summaries evaluation intrinsic extrinsic 36 inter-textual intra-textual 37 intrinsic evaluation assesses summaries directly extrinsic evaluation evaluates summarization system affects completion task intrinsic evaluations assessed mainly coherence informativeness summaries extrinsic evaluations hand tested impact summarization tasks like relevance assessment reading comprehension etc intra-textual evaluation assess output specific summarization system inter-textual evaluation focuses contrastive analysis outputs several summarization systems human judgement often varies greatly considers `` good '' summary creating automatic evaluation process particularly difficult manual evaluation used time labor-intensive requires humans read summaries also source documents issues concerning coherence coverage common way evaluate summaries rouge recall-oriented understudy gisting evaluation common summarization translation systems nist 's document understanding conferences 2 rouge recall-based measure well summary covers content human-generated summaries known references calculates n-gram overlaps automatically generated summaries previously written human summaries recall-based encourage inclusion important topics summaries recall computed respect unigram bigram trigram 4-gram matching example rouge-1 fraction unigrams appear reference summary automatic summary unigrams reference summary multiple reference summaries scores averaged high level overlap indicate high degree shared concepts two summaries rouge determine result coherent sentences flow together sensibly high-order n-gram rouge measures help degree another unsolved problem anaphor resolution similarly image summarization tschiatschek et al. developed visual-rouge score judges performance algorithms image summarization 38 domain-independent summarization techniques apply sets general features identify information-rich text segments recent research focuses domain-specific summarization using knowledge specific text 's domain medical knowledge ontologies summarizing medical texts 39 main drawback evaluation systems far need reference summary methods one compare automatic summaries models hard expensive task much effort made create corpora texts corresponding summaries furthermore methods require manual annotation summaries e.g scu pyramid method moreover perform quantitative evaluation regard different similarity metrics first publication area dates back 1957 40 hans peter luhn starting statistical technique research increased significantly 2015. term frequency–inverse document frequency used 2016. pattern-based summarization powerful option multi-document summarization found 2016. following year surpassed latent semantic analysis lsa combined non-negative matrix factorization nmf although replace approaches often combined 2019 machine learning methods dominated extractive summarization single documents considered nearing maturity 2020 field still active research shifting towards abstractive summation real-time summarization 41 recently rise transformer models replacing traditional rnn lstm provided flexibility mapping text sequences text sequences different type well suited automatic summarization includes models t5 42 pegasus 43 • none potthast martin hagen matthias stein benno 2016 author obfuscation attacking state art authorship verification conference labs evaluation forum • none roxana angheluta 2002 use topic segmentation automatic summarization • none elena lloret manuel palomar 2009 challenging issues automatic summarization relevance detection quality-based evaluation • none alrehamy hassan 2017 `` semcluster unsupervised automatic keyphrase extraction using affinity propagation '' automatic keyphrases extraction advances intelligent systems computing vol 650. pp 222–235 doi:10.1007/978-3-319-66939-7_19 isbn • none marcu daniel 2000 theory practice discourse parsing summarization isbn • published proceeding riao'10 adaptivity personalization fusion heterogeneous information cid paris france • none xiaojin zhu andrew goldberg jurgen van gael david andrzejewski 2007 improving diversity ranking using absorbing random walks grasshopper algorithm • none miranda-jiménez sabino gelbukh alexander sidorov grigori 2013 `` summarizing conceptual graphs automatic summarization task '' conceptual structures stem research education lecture notes computer science vol 7735. pp 245–253 doi:10.1007/978-3-642-35786-2_18 isbn conceptual structures stem research education\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4301a1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26891"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(formatted_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68283e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_summarize(text,number_of_sentences):\n",
    "    ogtxt=txt\n",
    "    formatted_text=preprocess(ogtxt)\n",
    "    \n",
    "    word_frequency=nltk.FreqDist(nltk.word_tokenize(formatted_text))\n",
    "    highest_frequency=max(word_frequency.values())\n",
    "    for word in word_frequency.keys():\n",
    "        word_frequency[word]=(word_frequency[word]/highest_frequency)\n",
    "    sentence_list=nltk.sent_tokenize(og_txt)\n",
    "    \n",
    "    score_sentences={}\n",
    "    for sentence in sentence_list:\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word in word_frequency.keys():\n",
    "                if sentence not in score_sentences.keys():\n",
    "                    score_sentences[sentence]=word_frequency[word]\n",
    "                else:\n",
    "                    score_sentences[sentence]+=word_frequency[word]\n",
    "    import heapq\n",
    "    best_sentences=heapq.nlargest(number_of_sentences,score_sentences,key=score_sentences.get)\n",
    "    \n",
    "    return sentence_list,best_sentences, word_frequency,score_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3565bb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Artificial Intelligence is human like intelligence.',\n",
       "  'It is the study of itelligent artificial agents.',\n",
       "  'Science and engineering to produce intelligent machines.',\n",
       "  'Solve problems and have intelligence.',\n",
       "  'Related to intelligent behavior.',\n",
       "  'Developing of reasoning machines.',\n",
       "  'Learn from mistakes and successes.',\n",
       "  'Artificial intelligence is related to reasoning in everyday situations'],\n",
       " ['Artificial intelligence is related to reasoning in everyday situations',\n",
       "  'Artificial Intelligence is human like intelligence.',\n",
       "  'It is the study of itelligent artificial agents.',\n",
       "  'Science and engineering to produce intelligent machines.',\n",
       "  'Solve problems and have intelligence.'],\n",
       " FreqDist({'intelligence': 1.0, 'mark': 0.75, 'artificial': 0.75, '/mark': 0.75, '.': 0.5, 'intelligent': 0.5, 'machines': 0.5, 'related': 0.5, 'reasoning': 0.5, 'human': 0.25, ...}),\n",
       " {'Artificial Intelligence is human like intelligence.': 2.0,\n",
       "  'It is the study of itelligent artificial agents.': 2.0,\n",
       "  'Science and engineering to produce intelligent machines.': 2.0,\n",
       "  'Solve problems and have intelligence.': 1.75,\n",
       "  'Related to intelligent behavior.': 1.25,\n",
       "  'Developing of reasoning machines.': 1.5,\n",
       "  'Learn from mistakes and successes.': 1.0,\n",
       "  'Artificial intelligence is related to reasoning in everyday situations': 2.5})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_summarize(article.cleaned_text,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc85ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=article.cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56c3c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list,best_sentences, word_frequency,score_sentences=frequency_summarize(k,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95555891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Artificial Intelligence is human like intelligence.': 2.0,\n",
       " 'It is the study of itelligent artificial agents.': 2.0,\n",
       " 'Science and engineering to produce intelligent machines.': 2.0,\n",
       " 'Solve problems and have intelligence.': 1.75,\n",
       " 'Related to intelligent behavior.': 1.25,\n",
       " 'Developing of reasoning machines.': 1.5,\n",
       " 'Learn from mistakes and successes.': 1.0,\n",
       " 'Artificial intelligence is related to reasoning in everyday situations': 2.5}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15945776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial intelligence is related to reasoning in everyday situations',\n",
       " 'Artificial Intelligence is human like intelligence.',\n",
       " 'It is the study of itelligent artificial agents.',\n",
       " 'Science and engineering to produce intelligent machines.',\n",
       " 'Solve problems and have intelligence.',\n",
       " 'Developing of reasoning machines.',\n",
       " 'Related to intelligent behavior.',\n",
       " 'Learn from mistakes and successes.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed50d2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6250845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Automatic_summarization\n",
      "https://en.wikipedia.org/wiki/Natural_language_processing\n",
      "https://en.wikipedia.org/wiki/Lemmatisation\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
